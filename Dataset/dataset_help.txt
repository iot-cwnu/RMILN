The existing bound-RFID HARs have the problems of limited recognized human activities, low recognition accuracy, and low classification ability for similar
activities. They are failure to consider analyzing and recognizing human activity from a multi-modal perspective. Based on the ability of body skeleton to 
fully simulate human activity and express key movement feature of activity, as well as the practical physical significance of RFID multi-modal, we propose a
bound-RFID multi-modal interactive learning network (RMILN HAR). In this study, the antenna (Larid S9028PCR) transmits radio frequency signal and receives 
signal responsed by RFID tag (Impinj M4E), while the reader (Impinj R420) obtains and records the responsed wireless signal and convert them into tag response
records related to human activity. Each tag response record includes EPC, timestamp, DF, RSSI, Phase, and the activity number indicating the current activity.
The collected tag response records of all activities constitute the human activity dataset (CWNU-RDM).
In order to increase the diversity and polymorphism of activity data, we recruited 15 volunteers with different physical conditions including a ratio of male
to female (8:6), a height range (1.55∼1.85 meters), and an age range (18∼30). We deployed 16 RFID tags on the body skeleton nodes of each volunteer according 
to the body RFID skeleton. Each volunteer finishes 21 human activities with forty times per activity. The tag response records collected during each time 
completed activity are called an activity instance. We obtained 12600 activity instances. We separately store the tag response records of all activities 
completed by a volunteer in a text file (15 volunteers, a total of 15 text files). Activity instances are separated by a blank line in each text file.
There are some very similar activities, such as “stride with swinging arms” and “steps with swing arms”, “stride without swinging arms” and “steps without 
swing arms”, as shown in https://github.com/iot-cwnu/RMILN.git for specific details. In order to achieve multi-modal HAR, we take three features (DF, RSSI,
and Phase) as three modalities.
